{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f63eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "# from CogDataset3d import *\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#seed_everything(11)\n",
    "\n",
    "from torchmetrics import R2Score as _r2score\n",
    "#from ignite.contrib.metrics.regression import R2Score\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd865a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b21cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'cleaned_df_5_31.csv'\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454971e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy = pd.read_csv(\"Healthy_with_volume.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd60cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'AGE', 'PTEDUCAT', 'APOE4', 'ADAS11', 'MMSE', 'ABETA_bl',\n",
       "       'TAU_bl', 'PTAU_bl', 'M', 'filenames', 'DX_bl_AD', 'DX_bl_CN',\n",
       "       'DX_bl_EMCI', 'DX_bl_LMCI', 'DX_bl_SMC', 'PTGENDER_Female',\n",
       "       'PTGENDER_Male', 'PTGENDER_nan', 'PTMARRY_Divorced', 'PTMARRY_Married',\n",
       "       'PTMARRY_Never married', 'Volume_BG', 'Volume_CSF', 'Volume_GM',\n",
       "       'Volume_WM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db56fce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AGE', 'PTEDUCAT', 'APOE4', 'ADAS11', 'MMSE', 'ABETA_bl', 'TAU_bl',\n",
       "       'PTAU_bl', 'M', 'filenames', 'DX_bl_AD', 'DX_bl_CN', 'DX_bl_EMCI',\n",
       "       'DX_bl_LMCI', 'DX_bl_SMC', 'PTGENDER_Female', 'PTGENDER_Male',\n",
       "       'PTGENDER_nan', 'PTMARRY_Divorced', 'PTMARRY_Married',\n",
       "       'PTMARRY_Never married'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b11f7",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca6658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Age3d(torch.utils.data.Dataset):\n",
    "   \n",
    "    \"\"\"\n",
    "    Class for getting individual transformations and data\n",
    "    Args:\n",
    "        input_dir = path of input images\n",
    "        target_dir = path of target images\n",
    "        input = list of filenames for input\n",
    "        target = list of filenames for target\n",
    "        transform = Images transformation (default: False)\n",
    "        crop = crop size\n",
    "        df = dataframe for cognitive scores\n",
    "    Output:\n",
    "        Transformed input\n",
    "        Transformed image target\n",
    "        ADAS11 score\n",
    "        MMSE score\n",
    "        filename\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dir, target_dir, input_files, df, transform=False, crop = (128,128,128)):\n",
    "        self.input_dir = input_dir \n",
    "        self.target_dir = target_dir\n",
    "        # sorted files in X_tr or X_v.\n",
    "        self.input = sorted(input_files)   \n",
    "        self.transform = transform\n",
    "        self.crop = crop\n",
    "        self.df = df\n",
    "        \n",
    "        # TODO: I need to change this into all healthy subjects.\n",
    "        patient_files = pickle.load(open(\"/home/madar/patient_files.data\", \"rb\"))\n",
    "        self.patient_files = list(set(map(lambda x: x[0], patient_files)))\n",
    "        \n",
    "        self.train_transforms = Compose([RandomCrop(shape = (128,128,128), always_apply=True),\n",
    "                                        ElasticTransform((0, 0.20), interpolation=4, p=1),\n",
    "                                         RandomRotate90((0,1), p=0.5),\n",
    "                                        #RandomGamma(gamma_limit=(0.5, 1.5), p=0.8),\n",
    "                                         Normalize(always_apply=True)], p=1.0)\n",
    "\n",
    "        self.val_transforms = Compose([CenterCrop(shape = (128,128,128), always_apply=True),\n",
    "                                       Normalize(always_apply=True)], p=1.0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        # grab the baseline images\n",
    "        X_tr_pid = list(map(lambda x: x[8:16], [self.input[i]]))\n",
    "        new_input = list(map(self.get_baseline_file, X_tr_pid))[0]\n",
    "        new_target = new_input.split('.nii')[0]+'_seg.nii'\n",
    "        \n",
    "        inp = nib.load(self.input_dir + new_input).get_fdata()\n",
    "        target = nib.load(self.target_dir + new_target).get_fdata()\n",
    "        \n",
    "        data = {'image': inp, 'mask': target}\n",
    "        \n",
    "        if self.transform == True:\n",
    "            aug_data = self.train_transforms(**data)\n",
    "            filename_df = self.input[i].split('.nii')[0]\n",
    "        else:\n",
    "            aug_data = self.val_transforms(**data)\n",
    "            filename_df = self.input[i].split('.nii')[0]\n",
    "\n",
    "        #checking if image has an associated cognitive score \n",
    "        files_have_cog = self.df['filenames'].values.tolist()\n",
    "        a_score = filename_df in files_have_cog\n",
    "        \n",
    "        #returning the cognitive score if true\n",
    "        y_age_score = None\n",
    "        if a_score == True:\n",
    "            y_age_score = self.df[self.df['filenames'] == filename_df]['AGE'].values[0]\n",
    "            \n",
    "        x, y_img = aug_data['image'], aug_data['mask']\n",
    "        \n",
    "        return x[None,], y_img, y_age_score, self.input[i].split('.nii')[0]\n",
    "    \n",
    "    def get_baseline_file(self, current_file):\n",
    "        for s in filter(lambda x: current_file in x, self.patient_files):\n",
    "            return s\n",
    "\n",
    "            \n",
    "def visualize_slices(brain, start, stop, target=False, slice_type='sagittal'):\n",
    "    \"\"\"\n",
    "    brain: instance of the dataset\n",
    "    start: starting slice\n",
    "    stop: ending slice\n",
    "    target: return input or target\n",
    "    slice_type: sagittal, coronal, or horizontal slices\n",
    "    \"\"\"\n",
    "    rang = stop-start\n",
    "    cols = int(rang/5)\n",
    "    \n",
    "    fig, ax = plt.subplots(cols, 5, figsize = (int(25),int(rang/(1.5))))\n",
    "    fig.set_facecolor(\"black\")\n",
    "    ax = ax.flatten()\n",
    "    start_idx = start\n",
    "\n",
    "    for i in range(0,rang, 1):\n",
    "        if slice_type == 'sagittal':            \n",
    "            brain_in = brain[0][:,start+i,:,:]\n",
    "            brain_out= brain[1][start+i,:,:]\n",
    "        elif slice_type == 'coronal':\n",
    "            brain_in = brain[0][:,:,start+i,:]\n",
    "            brain_out= brain[1][:,start+i,:]\n",
    "        elif slice_type == 'horizontal':\n",
    "            brain_in = brain[0][:,:,:,start+i]\n",
    "            brain_out= brain[1][:,:,start+i]\n",
    "\n",
    "        shape_img = np.shape(brain_in)\n",
    "        if target == False:\n",
    "            ax[i].set_facecolor('black')\n",
    "            ax[i].set_title(f'slice: {start_idx}')\n",
    "            ax[i].imshow(brain_in.reshape(shape_img[1],shape_img[2]))\n",
    "        if target == True:\n",
    "            ax[i].set_facecolor('black')\n",
    "            ax[i].set_title(f'slice: {start_idx}')\n",
    "            ax[i].imshow(brain_out)\n",
    "        start_idx+=1\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "def split_train_val(X_tr, X_v, df):\n",
    "    X_train_files = [f.split('.nii')[0] for f in X_tr]\n",
    "    X_val_files = [f.split('.nii')[0] for f in X_v]\n",
    "\n",
    "    X_train = df[df['filenames'].isin(X_train_files)]\n",
    "    X_val = df[df['filenames'].isin(X_val_files)]\n",
    "\n",
    "    y_age_train = X_train['AGE'].values\n",
    "    y_age_val = X_val['AGE'].values\n",
    "#     y_mmse_train = X_train['MMSE'].values\n",
    "#     y_mmse_val = X_val['MMSE'].values\n",
    "\n",
    "    X_train = X_train.drop(columns=['filenames', 'ADAS11', 'MMSE', 'AGE'])\n",
    "    X_val = X_val.drop(columns=['filenames', 'ADAS11', 'MMSE', 'AGE'])\n",
    "    \n",
    "    return X_train, X_val, y_age_train, y_age_val #, y_mmse_train, y_mmse_val\n",
    "\n",
    "\n",
    "def initialize_data():\n",
    "    input_path = '/media/rajlab/sachin_data_1/userdata/daren/mri/'\n",
    "    target_path = '/media/rajlab/sachin_data_1/userdata/daren/target/target_files/'\n",
    "    # Take only healthy subjs\n",
    "    csv_path = 'cleaned_df_5_31.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X_tr, X_v = get_file_splits()\n",
    "    print(f'len X_v: {len(X_v)}')\n",
    "    X_train, X_val, y_age_train, y_age_val= split_train_val(X_tr, X_v, df)\n",
    "    \n",
    "    return X_train, X_val, y_age_train, y_age_val, input_path, target_path, csv_path, df\n",
    "    \n",
    "    \n",
    "def get_file_splits(subset='all'):\n",
    "    if subset == 'all':\n",
    "        paths = ['/home/madar/Downloads/train_files5.data', \n",
    "                 '/home/madar/Downloads/val_files5.data'] \n",
    "    with open(paths[0], 'rb') as filehandle:\n",
    "        X_tr = pickle.load(filehandle)\n",
    "    with open(paths[1], 'rb') as filehandle:\n",
    "        X_v = pickle.load(filehandle)\n",
    "\n",
    "    return X_tr, X_v\n",
    "    \n",
    "\n",
    "def get_ds_dl(subset='all', batch_size=10, num_workers=16):\n",
    "    _, _, _, _, input_path, target_path, csv_path, df = initialize_data()\n",
    "    X_tr, X_v = get_file_splits(subset=subset) \n",
    "    ds_train = Age3d(input_path, target_path, X_tr, df, transform=True, crop = (128,128))\n",
    "    ds_val = Age3d(input_path, target_path, X_v, df, transform=False, crop = (128,128))\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    dl_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    return ds_train, ds_val, dl_train, dl_val\n",
    "\n",
    " # Not used for now.\n",
    "def tab_predict(pipe, X_train, y_train, X_val, y_val, name = 'Model'):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_val)\n",
    "    train_preds = pipe.predict(X_train)\n",
    "    print(f\"{f'{name} Train Loss'}: {round(mean_squared_error(y_train, train_preds),3)}\")\n",
    "    print(f\"{f'{name}  Train R2  '}: {round(r2_score(y_train, train_preds),3)}\\n\")\n",
    "    print(f\"{f'{name}  Valid Loss'}: {round(mean_squared_error(y_val, preds),3)}\")\n",
    "    print(f\"{f'{name}  Valid R2  '}: {round(r2_score(y_val, preds),3)}\\n\")\n",
    "    \n",
    "    return train_preds, preds\n",
    "\n",
    "\n",
    "def show_test_accuracy(nums, model, dl_test, batch_size=10, \n",
    "                       device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    use_amp = True\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    i=0\n",
    "    nums=1\n",
    "    for x, y, y_score, filenames in dl_test:\n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            y = y.squeeze(1).long().cuda()\n",
    "            dim1,dim2,dim3,dim4 = y.size() #CHANGED\n",
    "            x = x.view(dim1,1,dim2,dim3,dim4).cuda()\n",
    "            total += dim1*dim2*dim3*dim4  \n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp): \n",
    "\n",
    "                total += y.shape[0]\n",
    "                reg_out, y_hat = model(x)\n",
    "                loss = F.cross_entropy(y_hat, y)\n",
    "                batch_losses.append(loss.item())\n",
    "                pred = torch.max(y_hat, 1)[1]\n",
    "                correct += (pred == y).float().sum().item()   \n",
    "\n",
    "                if i < nums:\n",
    "#                     slice_idx = random.randint(40,100)\n",
    "                    slice_idx = 100\n",
    "                    fig, ax = plt.subplots(3,3, figsize=(10,10))\n",
    "#                     fig.set_facecolor(\"black\")\n",
    "                    ax=ax.flatten()\n",
    "                    sag_record = [x[i][0,:,:,slice_idx], y[i][:,:,slice_idx], pred[i][:,:,slice_idx]]\n",
    "                    hor_record = [x[i][0,:,slice_idx,:], y[i][:,slice_idx,:], pred[i][:,slice_idx,:]]\n",
    "                    cor_record = [x[i][0,slice_idx,:,:], y[i][slice_idx,:,:], pred[i][slice_idx,:,:]]\n",
    "\n",
    "                    for idx in range(0,3):\n",
    "                        colormap = [\"gray\", \"jet\", \"jet\"][idx]\n",
    "#                         ax[idx].set_facecolor('black')\n",
    "                        ax[idx].imshow((sag_record[idx]).cpu().numpy().reshape(128,128))\n",
    "#                         ax[idx+3].set_facecolor('black')\n",
    "                        ax[idx+3].imshow((hor_record[idx]).cpu().numpy().reshape(128,128))\n",
    "#                         ax[idx+6].set_facecolor('black')\n",
    "                        ax[idx+6].imshow((cor_record[idx]).cpu().numpy().reshape(128,128))\n",
    "                        \n",
    "                    i += 1\n",
    "    print(f'\\nCorrect predictions percentage is: {np.round((correct*100/total), 4)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453d387",
   "metadata": {},
   "source": [
    "## Model and Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9597c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "adevice = 'cuda'\n",
    "\n",
    "# please increase the epochs number\n",
    "PARAMS = {\n",
    "    'min_epochs': 20,\n",
    "    'max_epochs': 30,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 6,\n",
    "    'weight_decay' : 1e-3,\n",
    "    'extract_features' : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9c8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('cleaned_df_5_31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee8d01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1643, 21)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e02002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3d(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 batch_size=PARAMS['batch_size'],\n",
    "                 lr=PARAMS['learning_rate'],\n",
    "                 weight_decay=PARAMS['weight_decay']):\n",
    "        super(CNN3d, self).__init__()\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "        self.df = pd.read_csv('cleaned_df_5_31.csv')\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Define your 3D CNN model architecture\n",
    "        # \n",
    "        self.conv1 = nn.Conv3d(in_channels, 16, kernel_size=4, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv3d(64, 128, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm3d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        #self.fc = nn.Linear(128 * 1 * 1 * 1, 1)\n",
    "        self.fc = nn.Sequential(\n",
    "             nn.Flatten(),\n",
    "             nn.Linear(65536, 1024),  #1st hidden layer with 1024 units\n",
    "             nn.ReLU(),                          # Activation function for the 1st hidden layer\n",
    "             nn.Linear(1024, 512),               # 2nd hidden layer with 512 units\n",
    "             nn.ReLU(),                          # Activation function for the 2nd hidden layer\n",
    "             nn.Linear(512, 256),                # 3rd hidden layer with 256 units\n",
    "             nn.ReLU(),                          # Activation function for the 3rd hidden layer\n",
    "             nn.Linear(256, 1)                   # Output layer with 1 unit\n",
    "         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass of your 3D CNN model\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the features\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "        x, _, y, _ = batch\n",
    "        y_pred = self(x)\n",
    "        loss = nn.MSELoss(reduction = \"mean\")(y_pred.squeeze(), y.float())  # Use mean squared error loss for regression\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimizer\n",
    "        # Adam is a default one.\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define the validation step\n",
    "        x,_, y, _ = batch\n",
    "        y_pred = self(x)\n",
    "        val_loss = nn.MSELoss(reduction = \"mean\")(y_pred.squeeze(), y.float())\n",
    "        self.log('val_loss', val_loss, on_epoch=True)\n",
    "        return val_loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Create the data loader for training data\n",
    "        ds_train, ds_val, dl_train, dl_val = get_ds_dl('all', batch_size=self.batch_size, num_workers=16)\n",
    "        return dl_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        ds_train, ds_val, dl_train, dl_val = get_ds_dl('all', batch_size=self.batch_size, num_workers=16)\n",
    "        return dl_val\n",
    "        \n",
    "        # implement this so that we can have the average val_loss of a whole epoch\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Calculate the average validation loss across all batches\n",
    "        #avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "\n",
    "        # Log the average validation loss\n",
    "        #self.log('avg_val_loss', avg_val_loss, on_epoch=True, logger=True)\n",
    "        \n",
    "        #print(f'Epoch [{self.current_epoch}] - Avg Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        val_losses = []\n",
    "        \n",
    "        for val_out in outputs:\n",
    "            val_losses.append(val_out)\n",
    "        \n",
    "        avg_val_loss = (torch.stack(val_losses).mean())\n",
    "        \n",
    "        print(\"mean train val loss\")\n",
    "        print(avg_val_loss)\n",
    "        \n",
    "        self.log('avg_val_loss', avg_val_loss,  logger=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff992bf",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41215f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ProgressBar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5a573",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss_epoch',\n",
    "    dirpath='/home/madar/unet2021/models/lightning_models/with_tabular/',\n",
    "    # change the variable names here. or assign these names to actual outputs,\n",
    "#     filename = 'AGE_{val_r2_epoch:.4f}_{val_score_mse_loss_epoch:.2f}_{epoch:02d}',\n",
    "    filename = 'AGE_{avg_val_loss:.2f}_{epoch:02d}',\n",
    "    save_top_k=3,\n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "progressbar = ProgressBar()\n",
    "\n",
    "trainer = Trainer(\n",
    " #   fast_dev_run = True,\n",
    "    gpus=[0], \n",
    "#         auto_select_gpus=True,\n",
    "    #auto_lr_find=True,\n",
    "#         strategy='ddp',\n",
    "    precision=16,\n",
    "    #deterministic=True,\n",
    "    plugins=DDPPlugin(find_unused_parameters=True),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    min_epochs= PARAMS['min_epochs'],\n",
    "    max_epochs = PARAMS['max_epochs'],\n",
    "#         extract_features = PARAMS['extract_features']\n",
    "    #logger=neptune_logger\n",
    ")\n",
    "\n",
    "# Train a new model from scratch.\n",
    "model = CNN3d(1)\n",
    "\n",
    "# Once you have a decent model.\n",
    "# model = CNN3d.load_from_checkpoint(PATH_TO_MODEL)\n",
    "\n",
    "# trainer.tune(model)\n",
    "\n",
    "# Save the intermediate OrderedDict.\n",
    "trainer.fit(model)\n",
    "\n",
    "trainer = Trainer(\n",
    " #   fast_dev_run = True,\n",
    "    gpus=[0], \n",
    "#         auto_select_gpus=True,\n",
    "    #auto_lr_find=True,\n",
    "#         strategy='ddp',\n",
    "    precision=16,\n",
    "    #deterministic=True,\n",
    "    plugins=DDPPlugin(find_unused_parameters=True),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    min_epochs= PARAMS['min_epochs'],\n",
    "    max_epochs = PARAMS['max_epochs'],\n",
    "#         extract_features = PARAMS['extract_features']\n",
    "    #logger=neptune_logger\n",
    ")\n",
    "\n",
    "# Train a new model from scratch.\n",
    "model = CNN3d(1)\n",
    "\n",
    "# Once you have a decent model.\n",
    "# model = CNN3d.load_from_checkpoint(PATH_TO_MODEL)\n",
    "\n",
    "# trainer.tune(model)\n",
    "\n",
    "# Save the intermediate OrderedDict.\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fe1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
