{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom gradient function for squared log error\n",
    "def gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the gradient for squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n",
    "\n",
    "# Custom hessian function for squared log error\n",
    "def hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the hessian for squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return (-np.log1p(predt) + np.log1p(y) + 1) / np.power(predt + 1, 2)\n",
    "\n",
    "# Custom objective function for squared log error\n",
    "def c_squared_log(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''Squared Log Error objective. A simplified version for RMSLE used as objective function.'''\n",
    "    predt = np.copy(predt)  # Avoid modifying the original prediction in-place\n",
    "    predt[predt < -1] = -1 + 1e-6  # Clip negative predictions to avoid issues with log1p\n",
    "\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    \n",
    "    return grad, hess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for demonstration (use your actual dataset here)\n",
    "X_train = np.random.rand(100, 10)  # 100 samples, 10 features\n",
    "y_train = np.random.rand(100) * 10  # Target values between 0 and 10\n",
    "\n",
    "# Convert data into DMatrix (XGBoost's internal data structure)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Parameters for the XGBoost model\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': \"c_squared_log\",  # Use the custom squared log objective\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/41/2a2f5ed6c997367ab7055185cf66d536c228b15a12b8e112a274808f48b5/optuna-4.0.0-py3-none-any.whl (362kB)\n",
      "\u001b[K     |████████████████████████████████| 368kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorlog (from optuna)\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/18/3e867ab37a24fdf073c1617b9c7830e06ec270b1ea4694a624038fc40a03/colorlog-6.8.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from optuna) (1.19.5)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from optuna) (1.3.9)\n",
      "Requirement already satisfied: tqdm in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from optuna) (4.46.0)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/47/95d8f99c9f4a57079dfbcff5e023c5d81bde092d1c2354156340a56b3a1a/alembic-1.12.1-py3-none-any.whl (226kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: PyYAML in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from optuna) (5.1.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/3b/68690a035ba7347860f1b8c0cde853230ba69ff41df5884ea7d89fe68cd3/Mako-1.2.4-py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.9\" in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (4.8.2)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->optuna) (2.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from Mako->alembic>=1.5.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/darenma/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.9\"->alembic>=1.5.0->optuna) (3.15.0)\n",
      "Installing collected packages: colorlog, Mako, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.12.1 colorlog-6.8.2 optuna-4.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.train(params, dtrain, 1, obj=c_squared_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Define the custom gradient and hessian for squared log error\n",
    "def gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the gradient for squared log error.'''\n",
    "    y = dtrain.get_label()  # dtrain is a DMatrix object, get labels\n",
    "    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n",
    "\n",
    "def hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the hessian for squared log error.'''\n",
    "    y = dtrain.get_label()  # dtrain is a DMatrix object, get labels\n",
    "    return (-np.log1p(predt) + np.log1p(y) + 1) / np.power(predt + 1, 2)\n",
    "\n",
    "def squared_log(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    '''Squared Log Error objective function for XGBoost.'''\n",
    "    predt = np.copy(predt)  # Avoid modifying the original prediction in-place\n",
    "    predt[predt < -1] = -1 + 1e-6  # Clip negative predictions to avoid log1p issues\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess\n",
    "\n",
    "# Create synthetic dataset (replace this with your actual dataset)\n",
    "X = np.random.rand(400, 4)  # 400 samples, 4 features\n",
    "y = np.random.rand(400) * 10  # Target values between 0 and 10\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the training and validation sets to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.48133273e-09,  8.51373898e-09, -6.04478033e-09, -4.50914799e-09,\n",
       "        -2.19162715e-08, -4.82722279e-09,  5.80247690e-09, -8.58345292e-09,\n",
       "         1.19275949e-08, -6.54205090e-09,  5.57265052e-10, -1.39370479e-09,\n",
       "         1.48105424e-08, -1.32078136e-09, -1.03201216e-08,  6.45635101e-09,\n",
       "        -4.33696680e-09, -8.48865702e-09, -7.40097025e-09,  7.82187345e-09,\n",
       "        -9.22205574e-09,  3.55988228e-10, -8.58049863e-09,  3.07766280e-09,\n",
       "         1.43896021e-08,  1.11258692e-08, -5.79200849e-09,  9.87625771e-09,\n",
       "        -1.32469045e-08,  4.31646506e-09, -5.08854026e-09,  4.39375411e-09,\n",
       "         8.26143827e-10, -1.27925430e-09,  3.32509736e-09, -2.32680091e-10,\n",
       "        -9.91077440e-09,  2.88351512e-09,  1.09976692e-08,  1.23591854e-08,\n",
       "         1.21807292e-09,  4.93638305e-10, -9.93804982e-09, -7.94159996e-09,\n",
       "        -1.56263238e-08, -8.84101233e-09,  3.67424154e-09,  8.46316385e-10,\n",
       "        -2.72349910e-09, -6.92264356e-10,  5.28808099e-09,  9.76287483e-09,\n",
       "         5.32630371e-09, -9.95220039e-10,  3.37822498e-09,  1.34545213e-08,\n",
       "         1.24689842e-08,  8.30904376e-09, -8.36677111e-09,  1.33383903e-09,\n",
       "         1.65611183e-08, -3.93618866e-09, -8.28283518e-09, -3.98291905e-09,\n",
       "        -8.08489114e-09, -3.07117318e-09, -8.25336045e-09,  1.96042699e-09,\n",
       "        -1.70565038e-09,  1.16606758e-08,  1.53185228e-08,  1.00556010e-08,\n",
       "         1.15669954e-08,  2.56884644e-08, -1.01309428e-08,  2.96881066e-09,\n",
       "         1.72289038e-08,  1.44909537e-08, -2.29007358e-09,  1.10739259e-08,\n",
       "        -8.56616990e-09,  9.63133825e-10, -1.49115455e-08,  1.15826957e-08,\n",
       "         9.97596136e-09,  1.17184923e-09, -1.34693028e-08, -1.51712742e-08,\n",
       "        -1.90510270e-08, -1.31897064e-08,  9.67039473e-09, -1.97545112e-09,\n",
       "         1.97555709e-09, -9.28034640e-09,  1.11401168e-08, -9.57844459e-09,\n",
       "        -9.90936880e-09,  6.75901006e-09, -4.43285762e-09, -9.51731035e-09,\n",
       "        -1.10547557e-09,  1.24061065e-08, -6.95435055e-09,  5.41491646e-09,\n",
       "         5.24237157e-09, -7.09330447e-09,  9.25747068e-09,  4.79113580e-09,\n",
       "        -1.45602870e-08,  1.36389497e-08,  4.92742000e-09, -9.24986771e-09,\n",
       "         1.43586396e-09,  6.11440320e-09, -5.51979394e-09, -1.15121842e-08,\n",
       "        -3.12693081e-09, -2.54936248e-09,  1.11667924e-08,  1.05287585e-08,\n",
       "         6.92127598e-09,  6.72629606e-09, -1.47420123e-08, -5.64706116e-09,\n",
       "         3.33470444e-09, -1.04518327e-08, -8.97074997e-09,  1.17669407e-08,\n",
       "        -4.68343863e-09, -1.84144224e-08, -8.66981275e-09, -1.27341189e-09,\n",
       "         7.42811587e-09,  1.25828524e-08, -7.31655943e-10,  1.56676404e-08,\n",
       "         1.32885405e-08,  4.44411552e-09, -1.90724717e-09, -8.67563637e-10,\n",
       "         8.51748505e-09,  8.62171561e-09, -1.57684072e-09, -6.14033758e-09,\n",
       "        -7.64404985e-09,  1.22026091e-08,  2.10616847e-08,  7.41717748e-09,\n",
       "        -1.10784543e-08, -7.12530268e-09, -1.34354336e-08, -1.64142850e-08,\n",
       "         8.40234617e-09, -9.81313322e-10,  1.11694982e-08, -4.88358001e-09,\n",
       "        -9.60116371e-09, -2.15645490e-09, -2.37429892e-09, -7.25858829e-09,\n",
       "        -4.82418618e-09, -4.52862422e-09, -6.99208973e-11,  9.59080396e-09,\n",
       "        -6.82333566e-09,  1.50171994e-08,  8.05182743e-09, -8.38822692e-09,\n",
       "        -4.31342997e-09, -9.28742551e-10,  6.62722168e-09,  2.40045634e-09,\n",
       "        -1.02249284e-08, -2.49471890e-09,  1.26913498e-08, -2.80897374e-09,\n",
       "        -3.37837285e-09,  7.74121047e-09, -8.89595556e-09, -6.22739680e-09,\n",
       "        -1.85951804e-09,  8.11955856e-10, -6.32838322e-09, -6.48456042e-09,\n",
       "        -1.55118560e-08, -1.06845160e-08,  1.56160267e-09, -6.75225693e-09,\n",
       "         1.25544822e-08, -7.53560499e-09,  9.10448123e-09, -4.04005000e-09,\n",
       "        -7.25891243e-09, -1.40079973e-08,  1.60418819e-08, -1.84326273e-10,\n",
       "         3.56798391e-09,  1.89363445e-08, -1.24045192e-08,  3.04365826e-09,\n",
       "        -1.58019098e-08, -1.28836208e-09, -5.84638458e-09, -4.25188188e-09,\n",
       "         2.98763432e-10,  1.51425681e-08,  2.73778944e-09, -6.55465359e-09,\n",
       "        -1.38397054e-08, -6.69358208e-10,  3.24166201e-09,  1.01866458e-08,\n",
       "        -2.58240673e-09,  2.72818985e-09, -3.77721190e-09,  3.31244063e-10,\n",
       "        -6.48708268e-09, -7.39940014e-09, -9.29856700e-09,  2.63157838e-09,\n",
       "        -1.38511912e-09,  9.31154753e-09, -4.69190339e-09,  1.26493828e-08,\n",
       "         1.12214341e-08,  7.51056991e-11, -6.23600469e-10, -9.68867071e-09,\n",
       "        -7.81675868e-09,  8.57881491e-09,  5.91984871e-09, -5.75058786e-09,\n",
       "        -7.95703206e-09,  5.53836657e-09, -4.75732833e-09, -4.97909712e-09,\n",
       "        -8.96298898e-09, -9.66167847e-09,  5.76059866e-09, -1.03679320e-08,\n",
       "        -1.87043978e-10,  1.75167111e-08, -1.34604508e-08,  1.26218438e-08,\n",
       "        -1.43783518e-08,  9.72186952e-10,  6.57311123e-09,  5.07172178e-09,\n",
       "        -1.49518065e-08, -1.68809936e-08, -5.52217430e-09,  4.10636855e-10,\n",
       "         2.32043533e-08, -5.73448497e-09,  2.97526991e-09,  7.80572021e-09,\n",
       "        -2.92287201e-09, -6.69548436e-09,  8.56406100e-09, -1.10931514e-08,\n",
       "         4.63625369e-09,  1.99757882e-09, -6.01814115e-09, -1.15529283e-08,\n",
       "         4.48711731e-09,  4.79627222e-09,  1.34874274e-08, -4.95175862e-10,\n",
       "         4.02891007e-09, -3.34784063e-11,  1.06595731e-08, -3.12304441e-09,\n",
       "        -1.33916684e-08, -9.88114493e-09,  1.66409467e-09,  8.14301353e-09,\n",
       "        -1.62396194e-08, -4.42378488e-09,  2.88422925e-09, -1.22590100e-08,\n",
       "        -1.20170011e-08,  3.23330604e-09,  1.95269210e-09, -1.03305282e-08,\n",
       "         7.22056448e-09, -4.65669528e-09,  6.24045965e-10,  4.38767524e-09,\n",
       "         5.94680387e-09,  7.85332714e-09, -6.02759464e-09,  1.13889844e-08,\n",
       "        -9.80680445e-09, -5.77838395e-09, -1.76481498e-08, -7.01600844e-09,\n",
       "        -1.13157682e-08, -4.17833102e-09,  8.46683205e-09, -8.83086956e-09,\n",
       "         1.60945813e-08,  6.09032125e-09,  1.30333136e-08,  1.05534416e-08,\n",
       "        -9.53436453e-09, -8.61771127e-09, -1.05250619e-09,  7.29592299e-09,\n",
       "        -1.16350891e-08,  5.67282542e-09,  3.41661049e-09, -8.14290474e-09,\n",
       "        -2.80514673e-09, -1.31329227e-08, -6.86434666e-10, -6.19926900e-09,\n",
       "        -9.21945019e-10, -2.24054707e-09, -4.59218389e-09, -9.28739459e-09]),\n",
       " array([0.55018909, 0.01005122, 0.80207481, 0.12003591, 0.11719219,\n",
       "        0.00851958, 0.08318685, 0.05434301, 0.01369237, 0.01059395,\n",
       "        0.23906255, 0.03149087, 0.01633369, 0.8219511 , 0.07262544,\n",
       "        0.01648825, 0.01203877, 0.05464076, 0.17438684, 0.34890494,\n",
       "        0.01021907, 0.79097741, 0.07468116, 0.25340068, 0.24176899,\n",
       "        0.00940233, 0.00923964, 0.00868511, 0.01757274, 0.21027761,\n",
       "        0.06544819, 0.03306969, 0.1941701 , 0.04232691, 0.03371091,\n",
       "        0.01536509, 0.01114873, 0.01530794, 0.00992341, 0.02903645,\n",
       "        0.02693936, 0.0274301 , 0.13820349, 0.00942497, 0.084768  ,\n",
       "        0.01165805, 0.01909034, 0.69171376, 0.03834884, 0.0178568 ,\n",
       "        0.05152613, 0.3710252 , 0.01971634, 0.02123559, 0.05767007,\n",
       "        0.0272257 , 0.02596918, 0.09200457, 0.1261257 , 0.05325879,\n",
       "        0.0709    , 0.05501636, 0.11075433, 0.02967469, 0.04843344,\n",
       "        0.00983675, 0.01089901, 0.04234144, 0.78449697, 0.02297469,\n",
       "        0.09898186, 0.01117397, 0.01070464, 0.24056527, 0.01029177,\n",
       "        0.01063398, 0.06169401, 0.32615583, 0.05279391, 0.01311763,\n",
       "        0.05339206, 0.13558928, 0.07327137, 0.01193764, 0.03553319,\n",
       "        0.00832163, 0.01340536, 0.02980405, 0.17340215, 0.12633966,\n",
       "        0.03387392, 0.19145659, 0.0262981 , 0.12825254, 0.01260769,\n",
       "        0.02588799, 0.01584157, 0.01611635, 0.02191763, 0.07531861,\n",
       "        0.01580538, 0.01675491, 0.01518864, 0.20355608, 0.02952343,\n",
       "        0.01395944, 0.01048796, 0.0448406 , 0.02958009, 0.13291737,\n",
       "        0.01613157, 0.1119491 , 0.01061122, 0.02553813, 0.02967627,\n",
       "        0.40552391, 0.03204111, 0.06406662, 0.03563747, 0.05367345,\n",
       "        0.09709523, 0.04198609, 0.44428941, 0.04808057, 0.60714432,\n",
       "        0.06536977, 0.01598844, 0.00996159, 0.01298197, 0.11732266,\n",
       "        0.02692414, 0.00851901, 0.01212191, 0.00944715, 0.00879695,\n",
       "        0.25110974, 0.05764604, 0.01497305, 0.05683268, 0.01152384,\n",
       "        0.15639757, 0.05981853, 0.0129564 , 0.04364241, 0.16928465,\n",
       "        0.03793598, 0.08257254, 0.04131107, 0.02714269, 0.01120235,\n",
       "        0.01516925, 0.01790674, 0.01629009, 0.13773004, 0.01378739,\n",
       "        0.02678341, 0.01273534, 0.04947079, 0.09408597, 0.01023467,\n",
       "        0.05180145, 0.62246407, 0.02569808, 0.0087608 , 0.01201008,\n",
       "        0.04571009, 0.0677678 , 0.00870337, 0.01143759, 0.46240831,\n",
       "        0.02143935, 0.02049549, 0.11284119, 0.0554439 , 0.01525864,\n",
       "        0.2243825 , 0.04480199, 0.07900077, 0.02472983, 0.0106343 ,\n",
       "        0.01811554, 0.29747967, 0.45375305, 0.06850134, 0.07794124,\n",
       "        0.02609984, 0.95156614, 0.0294236 , 0.22566013, 0.08004054,\n",
       "        0.01556189, 0.01606129, 0.01756528, 0.01213546, 0.08310637,\n",
       "        0.14264035, 0.0104642 , 0.07633191, 0.031038  , 0.00993607,\n",
       "        0.08590486, 0.34756587, 0.01229989, 0.7717141 , 0.01076256,\n",
       "        0.01579436, 0.18918448, 0.18607674, 0.06986981, 0.01253859,\n",
       "        0.02896062, 0.05123683, 0.72299203, 0.01734543, 0.01359207,\n",
       "        0.38056095, 0.00906171, 0.0199257 , 0.04256193, 0.15643746,\n",
       "        0.035707  , 0.00957941, 0.03307594, 0.02125036, 0.06559635,\n",
       "        0.01050967, 0.01148024, 0.01294941, 0.05892169, 0.03104654,\n",
       "        0.52157949, 0.24942099, 0.04561816, 0.00952196, 0.01466341,\n",
       "        0.05798524, 0.04788635, 0.13097262, 0.02027741, 0.00828396,\n",
       "        0.01150688, 0.03778302, 0.01134763, 0.11398163, 0.01338208,\n",
       "        0.03780651, 0.02223453, 0.05095794, 0.05806268, 0.05041664,\n",
       "        0.01001224, 0.47596908, 0.09436073, 0.01047239, 0.01465753,\n",
       "        0.01669418, 0.01062287, 0.01038535, 0.30237227, 0.06176964,\n",
       "        0.00855008, 0.02494843, 0.01874316, 0.05743605, 0.0278147 ,\n",
       "        0.03413861, 0.08671569, 0.25349012, 0.02228833, 0.00864046,\n",
       "        0.11746518, 0.01151521, 0.01246835, 0.14703211, 0.02175426,\n",
       "        0.63151229, 0.06236643, 0.07488713, 0.10605696, 0.02236726,\n",
       "        0.25980943, 0.02861773, 0.02418307, 0.01038658, 0.03629621,\n",
       "        0.01179653, 0.78362704, 0.01838527, 0.0187394 , 0.05000376,\n",
       "        0.02123769, 0.13879203, 0.45716286, 0.04123001, 0.26895086,\n",
       "        0.01360618, 0.48809904, 0.01156517, 0.2449152 , 0.09117023,\n",
       "        0.11867418, 0.0117138 , 0.04541428, 0.02596539, 0.02193522,\n",
       "        0.01979513, 0.00920787, 0.01222487, 0.04528897, 0.0105469 ,\n",
       "        0.71799999, 0.0100892 , 0.01183127, 0.01649927, 0.17076535,\n",
       "        0.00899375, 0.04866245, 0.01113307, 0.09730126, 0.03681189]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_log(y_train, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-29 00:34:34,315] A new study created in memory with name: no-name-40cfd897-3350-4aa0-9811-d708630c7308\n"
     ]
    }
   ],
   "source": [
    "# Define an objective function for Optuna to optimize\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to be optimized\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'objective': squared_log,  # Custom squared_log objective\n",
    "        'eval_metric': 'mae',      # Mean Absolute Error for evaluation\n",
    "    }\n",
    "\n",
    "    # Train the model using DMatrix\n",
    "    model = xgb.train(param, dtrain, num_boost_round=10, evals=[(dval, \"validation\")], \n",
    "                      obj=squared_log, early_stopping_rounds=1, verbose_eval=False)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    preds = model.predict(dval)\n",
    "    \n",
    "    # Compute the Mean Absolute Error (MAE) on the validation set\n",
    "    error = mean_absolute_error(y_val, preds)\n",
    "    \n",
    "    return error\n",
    "\n",
    "# Create a study object and run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print('Best parameters:', study.best_params_)\n",
    "print('Best score:', study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
